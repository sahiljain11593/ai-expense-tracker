#!/usr/bin/env python3
"""
Bank Statement Analyzer and Cross-Checker

This tool provides comprehensive analysis and cross-checking of bank statements
from CSV and PDF sources, with intelligent categorization and reconciliation.
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BankStatementAnalyzer:
    """Comprehensive bank statement analysis and cross-checking system."""
    
    def __init__(self):
        self.csv_data = None
        self.pdf_data = None
        self.reconciliation_results = {}
        self.categorization_rules = self._load_categorization_rules()
        
    def _load_categorization_rules(self) -> Dict[str, List[str]]:
        """Load comprehensive categorization rules for Japanese bank statements."""
        return {
            "Food & Groceries": [
                # Convenience stores
                "ãƒ­ãƒ¼ã‚½ãƒ³", "ã‚»ãƒ–ãƒ³ã‚¤ãƒ¬ãƒ–ãƒ³", "ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒãƒ¼ãƒˆ", "ã‚³ãƒ³ãƒ“ãƒ‹", "lawson", "seven eleven", "family mart",
                "ãƒãƒ—ãƒ©ã‚°ãƒ«ãƒ¼ãƒ—", "poplar", "ã‚¹ãƒ¼ãƒ‘ãƒ¼", "supermarket", "grocery", "market", "food", "fresh",
                "ã‚¤ã‚ªãƒ³", "aeon", "ã‚¤ãƒˆãƒ¼ãƒ¨ãƒ¼ã‚«ãƒ‰ãƒ¼", "itoyokado", "è¥¿å‹", "seiyu", "ãƒ©ã‚¤ãƒ•", "life",
                # Restaurants and dining
                "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "restaurant", "cafe", "dinner", "lunch", "breakfast", "takeaway", "delivery",
                "å±…é…’å±‹", "izakaya", "ãƒãƒ¼", "bar", "ã‚«ãƒ•ã‚§", "coffee", "ãƒ”ã‚¶", "pizza", "å¯¿å¸", "sushi",
                "ãƒã‚¯ãƒ‰ãƒŠãƒ«ãƒ‰", "mcdonalds", "ã‚±ãƒ³ã‚¿ãƒƒã‚­ãƒ¼", "kfc", "ã‚¹ã‚¿ãƒ¼ãƒãƒƒã‚¯ã‚¹", "starbucks",
                # Specific merchants
                "ãƒ‰ãƒ³ã‚­ãƒ›ãƒ¼ãƒ†", "ãƒãƒ¼ã‚¬ãƒ¼ã‚­ãƒ³ã‚°", "ã‚¤ãƒ¼ã‚¸ãƒ¼ã‚ºã‚«ãƒ•ã‚§", "ã‚µã‚«ãƒ‰ãƒ¤", "ã‚¨ãƒŒã‚·ãƒ¼ãƒ‡ã‚¤",
                "ãƒãƒŠãƒãƒ«ã‚­", "ãƒãƒŠã‚³ã‚¦ã‚¸ãƒ§ã‚¦", "ã‚µã‚¦ãƒŠæ±äº¬", "ãƒãƒ«ã‚¨ãƒ„", "ãƒãƒ„ãƒãƒ¤ãƒ¨ã‚¦ã‚¬",
                "ã‚¬ãƒ³ã‚½ã‚ºã‚·", "ãƒãƒ«ã‚¬ãƒ¡ã‚»ã‚¤ãƒ¡ãƒ³", "ã‚«ãƒ–ã‚·ã‚­ã‚¬ã‚¤ã‚·ãƒ£", "ã‚µãƒ³ãƒˆãƒ©ãƒƒã‚¯", "ãƒãƒ–ãƒ«ãƒã‚¦ã‚¹"
            ],
            "Transportation": [
                # Public transport
                "é›»è»Š", "train", "ãƒã‚¹", "bus", "ã‚¿ã‚¯ã‚·ãƒ¼", "taxi", "åœ°ä¸‹é‰„", "subway", "ãƒ¢ãƒãƒ¬ãƒ¼ãƒ«", "monorail",
                "ãƒ¢ãƒã‚¤ãƒ«ãƒ‘ã‚¹", "mobile pass", "äº¤é€šè²»", "transport", "é§è»Šå ´", "parking", "é«˜é€Ÿé“è·¯", "highway",
                "ï¼¥ï¼´ï¼£", "etc", "ã‚¬ã‚½ãƒªãƒ³", "gasoline", "ç‡ƒæ–™", "fuel", "è»Š", "car", "ãƒã‚¤ã‚¯", "bike",
                # Specific merchants
                "ï¼¥ï¼®ï¼¥ï¼¯ï¼³", "ENEOS", "ã‚¬ã‚½ãƒªãƒ³ã‚¹ã‚¿ãƒ³ãƒ‰", "gas station"
            ],
            "Shopping & Retail": [
                # Online shopping
                "AMAZON", "amazon", "ã‚¢ãƒã‚¾ãƒ³", "æ¥½å¤©", "rakuten", "ãƒ¤ãƒ•ãƒ¼", "yahoo",
                # Department stores and retail
                "ãƒ‹ãƒˆãƒª", "nitori", "ã‚¤ã‚±ã‚¢", "ikea", "ãƒ›ãƒ¼ãƒ ã‚»ãƒ³ã‚¿ãƒ¼", "home center", "å®¶å…·", "furniture",
                "ãƒ¦ãƒ‹ã‚¯ãƒ­", "uniqlo", "zara", "h&m", "gap", "nike", "adidas", "ã‚¢ãƒ‡ã‚£ãƒ€ã‚¹", "ãƒŠã‚¤ã‚­",
                "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "fashion", "ã‚¹ã‚¿ã‚¤ãƒ«", "style", "ãƒ–ãƒ©ãƒ³ãƒ‰", "brand",
                # Specific merchants
                "ãƒã‚±ãƒƒãƒˆã‚¸ãƒ£ãƒ ", "ãƒ­ã‚³ãƒãƒ­ã‚¤", "ãƒã‚¯ãƒ†ã‚¤ãƒ—ãƒ©ã‚¶", "ãƒãƒ«ã‚¤ãƒ•ã‚¡", "æ±äº¬ãƒŸãƒƒãƒ‰ã‚¿ã‚¦ãƒ³",
                "æ–°ãƒãƒ«ãƒã‚¦ãƒãƒ“ãƒ«", "ã‚¿ã‚¤ãƒ ã‚ºã‚«ãƒ¼", "ã‚¤ãƒ‡ãƒŸãƒ„", "ã‚¢ãƒãƒ­ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"
            ],
            "Subscriptions & Services": [
                # Digital services
                "OPENAI", "CHATGPT", "icloud", "apple music", "amazon prime", "google one", 
                "netflix", "spotify", "hulu", "disney+", "ã‚¢ãƒã‚¾ãƒ³ãƒ—ãƒ©ã‚¤ãƒ ", "ã‚°ãƒ¼ã‚°ãƒ«ãƒ¯ãƒ³",
                "ã‚¢ãƒƒãƒ—ãƒ«ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "ã‚¢ã‚¤ã‚¯ãƒ©ã‚¦ãƒ‰", "subscription", "membership", "æœˆé¡", "monthly", "å¹´é¡", "annual",
                # Communication services
                "æ¥½å¤©ãƒ¢ãƒã‚¤ãƒ«", "rakuten mobile", "é€šä¿¡æ–™", "communication", "é›»è©±", "phone",
                # Utilities
                "æ¥½å¤©ã§ã‚“ã", "rakuten electricity", "æ¥½å¤©ã‚¬ã‚¹", "rakuten gas", "é›»æ°—", "electric", "ã‚¬ã‚¹", "gas"
            ],
            "Entertainment & Recreation": [
                # Entertainment venues
                "ã‚¤ãƒ™ãƒ³ãƒˆã‚´ãƒªãƒ§ã‚¦ãƒ–ãƒ³", "event", "ã‚¤ãƒ™ãƒ³ãƒˆ", "ã‚³ãƒ³ã‚µãƒ¼ãƒˆ", "concert", "æ˜ ç”»", "movie",
                "ã‚²ãƒ¼ãƒ ", "game", "ã‚¹ãƒãƒ¼ãƒ„", "sports", "ã‚«ãƒ©ã‚ªã‚±", "karaoke", "ãƒœãƒ¼ãƒªãƒ³ã‚°", "bowling",
                # Specific venues
                "ã‚¾ã‚¾ãƒãƒªãƒ³ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ", "ãƒãƒŠãƒãƒ«ã‚¦ãƒ‰ãƒ³", "ãƒŸãƒ‹ã‚¹ãƒˆãƒƒãƒ—", "ã‚¤ãƒ™ãƒ³ãƒˆã‚´ãƒªãƒ§ã‚¦ãƒ–ãƒ³"
            ],
            "Health & Wellness": [
                # Healthcare
                "ç—…é™¢", "hospital", "ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "clinic", "æ­¯ç§‘", "dental", "çœ¼ç§‘", "eye", "è–¬å±€", "pharmacy",
                "è–¬", "medicine", "ä¿é™º", "insurance", "è¨ºå¯Ÿ", "examination", "æ²»ç™‚", "treatment",
                "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹", "fitness", "ã‚¸ãƒ ", "gym", "ãƒ¨ã‚¬", "yoga", "ãƒãƒƒã‚µãƒ¼ã‚¸", "massage"
            ],
            "Fees & Charges": [
                # Bank fees
                "æ‰‹æ•°æ–™", "fee", "å¹´ä¼šè²»", "annual fee", "åˆ©ç”¨æ–™", "usage fee", "ï¼¡ï¼´ï¼­", "ATM",
                "æ”¯æ‰•æ‰‹æ•°æ–™", "payment fee", "ç¾åœ°åˆ©ç”¨é¡", "local usage", "å¤‰æ›ãƒ¬ãƒ¼ãƒˆ", "exchange rate"
            ],
            "Cash & ATM": [
                # Cash withdrawals
                "ãƒ­ãƒ¼ã‚½ãƒ³éŠ€è¡Œ", "lawson bank", "ï¼£ï¼¤", "CD", "ç¾é‡‘", "cash", "å¼•ãå‡ºã—", "withdrawal"
            ]
        }
    
    def parse_csv_statement(self, csv_path: str) -> pd.DataFrame:
        """Parse Japanese bank statement CSV file."""
        try:
            # Read CSV with proper encoding
            df = pd.read_csv(csv_path, encoding='utf-8')
            
            # Clean column names
            df.columns = df.columns.str.strip()
            
            # Map Japanese columns to English
            column_mapping = {
                'åˆ©ç”¨æ—¥': 'date',
                'åˆ©ç”¨åº—åãƒ»å•†å“å': 'description',
                'åˆ©ç”¨è€…': 'user',
                'æ”¯æ‰•æ–¹æ³•': 'payment_method',
                'åˆ©ç”¨é‡‘é¡': 'amount',
                'æ”¯æ‰•æ‰‹æ•°æ–™': 'fee',
                'æ”¯æ‰•ç·é¡': 'total_amount',
                '9æœˆæ”¯æ‰•é‡‘é¡': 'september_payment',
                '10æœˆç¹°è¶Šæ®‹é«˜': 'october_balance',
                'æ–°è¦ã‚µã‚¤ãƒ³': 'new_sign'
            }
            
            df = df.rename(columns=column_mapping)
            
            # Clean and convert data
            df = self._clean_transaction_data(df)
            
            logger.info(f"Successfully parsed CSV: {len(df)} transactions")
            return df
            
        except Exception as e:
            logger.error(f"Error parsing CSV: {e}")
            raise
    
    def parse_pdf_statement(self, pdf_path: str) -> pd.DataFrame:
        """Parse bank statement PDF file."""
        try:
            import pdfplumber
            
            transactions = []
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages[:10]:  # Process first 10 pages
                    tables = page.extract_tables()
                    for table in tables:
                        if len(table) > 1 and len(table[0]) >= 3:
                            # Try to identify transaction rows
                            for row in table[1:]:
                                if len(row) >= 3 and row[0] and row[1] and row[2]:
                                    try:
                                        # Parse date
                                        date_str = str(row[0]).strip()
                                        if re.match(r'\d{4}/\d{2}/\d{2}', date_str):
                                            date = datetime.strptime(date_str, '%Y/%m/%d')
                                            
                                            # Parse description
                                            description = str(row[1]).strip()
                                            
                                            # Parse amount
                                            amount_str = str(row[2]).strip()
                                            amount = float(re.sub(r'[^\d.-]', '', amount_str))
                                            
                                            transactions.append({
                                                'date': date,
                                                'description': description,
                                                'amount': amount,
                                                'source': 'PDF'
                                            })
                                    except Exception as e:
                                        continue
            
            df = pd.DataFrame(transactions)
            if not df.empty:
                df = self._clean_transaction_data(df)
            
            logger.info(f"Successfully parsed PDF: {len(df)} transactions")
            return df
            
        except Exception as e:
            logger.error(f"Error parsing PDF: {e}")
            raise
    
    def _clean_transaction_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize transaction data."""
        # Remove empty rows
        df = df.dropna(subset=['date', 'description', 'amount'])
        
        # Convert date to datetime
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # Clean amounts
        df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
        
        # Remove rows with invalid data
        df = df.dropna(subset=['date', 'amount'])
        
        # Add transaction type
        df['transaction_type'] = 'Expense'  # Most Japanese bank transactions are expenses
        
        # Add unique ID
        df['transaction_id'] = range(len(df))
        
        return df
    
    def cross_check_statements(self, csv_data: pd.DataFrame, pdf_data: pd.DataFrame) -> Dict:
        """Cross-check CSV and PDF statements for accuracy."""
        results = {
            'csv_summary': self._get_data_summary(csv_data, 'CSV'),
            'pdf_summary': self._get_data_summary(pdf_data, 'PDF'),
            'discrepancies': [],
            'reconciliation_status': 'unknown'
        }
        
        # Find transactions only in CSV
        csv_only = self._find_unique_transactions(csv_data, pdf_data, 'CSV')
        results['csv_only'] = csv_only
        
        # Find transactions only in PDF
        pdf_only = self._find_unique_transactions(pdf_data, csv_data, 'PDF')
        results['pdf_only'] = pdf_only
        
        # Calculate reconciliation status
        csv_total = csv_data['amount'].sum()
        pdf_total = pdf_data['amount'].sum()
        difference = abs(csv_total - pdf_total)
        
        if difference < 1000:  # Within Â¥1,000
            results['reconciliation_status'] = 'reconciled'
        elif difference < 10000:  # Within Â¥10,000
            results['reconciliation_status'] = 'minor_discrepancy'
        else:
            results['reconciliation_status'] = 'major_discrepancy'
        
        results['total_difference'] = difference
        results['csv_total'] = csv_total
        results['pdf_total'] = pdf_total
        
        return results
    
    def _get_data_summary(self, df: pd.DataFrame, source: str) -> Dict:
        """Get summary statistics for a dataset."""
        if df.empty:
            return {'source': source, 'count': 0, 'total': 0, 'date_range': 'N/A'}
        
        return {
            'source': source,
            'count': len(df),
            'total': df['amount'].sum(),
            'abs_total': df['amount'].abs().sum(),
            'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}",
            'duplicates': len(df) - len(df.drop_duplicates(subset=['date', 'description', 'amount']))
        }
    
    def _find_unique_transactions(self, df1: pd.DataFrame, df2: pd.DataFrame, source: str) -> List[Dict]:
        """Find transactions unique to one dataset."""
        if df1.empty or df2.empty:
            return df1.to_dict('records') if not df1.empty else []
        
        # Create comparison keys
        df1['key'] = df1['date'].astype(str) + '|' + df1['description'] + '|' + df1['amount'].astype(str)
        df2['key'] = df2['date'].astype(str) + '|' + df2['description'] + '|' + df2['amount'].astype(str)
        
        # Find unique transactions
        unique_mask = ~df1['key'].isin(df2['key'])
        unique_df = df1[unique_mask].copy()
        
        # Remove the key column
        unique_df = unique_df.drop('key', axis=1)
        
        return unique_df.to_dict('records')
    
    def categorize_transactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """Categorize transactions using intelligent rules."""
        df = df.copy()
        categories = []
        confidences = []
        
        for _, row in df.iterrows():
            description = str(row['description']).lower()
            category, confidence = self._categorize_transaction(description)
            categories.append(category)
            confidences.append(confidence)
        
        df['category'] = categories
        df['confidence'] = confidences
        
        return df
    
    def _categorize_transaction(self, description: str) -> Tuple[str, float]:
        """Categorize a single transaction with confidence score."""
        description = description.lower()
        
        # Check each category
        for category, keywords in self.categorization_rules.items():
            matches = sum(1 for keyword in keywords if keyword.lower() in description)
            if matches > 0:
                confidence = min(0.9, 0.5 + (matches * 0.1))
                return category, confidence
        
        return "Uncategorised", 0.0
    
    def generate_analysis_report(self, csv_data: pd.DataFrame, pdf_data: pd.DataFrame) -> str:
        """Generate comprehensive analysis report."""
        report = []
        report.append("# Bank Statement Analysis Report")
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Cross-check results
        cross_check = self.cross_check_statements(csv_data, pdf_data)
        
        report.append("## Summary")
        report.append(f"- CSV Transactions: {cross_check['csv_summary']['count']} | Total: Â¥{cross_check['csv_summary']['total']:,.0f}")
        report.append(f"- PDF Transactions: {cross_check['pdf_summary']['count']} | Total: Â¥{cross_check['pdf_summary']['total']:,.0f}")
        report.append(f"- Reconciliation Status: {cross_check['reconciliation_status']}")
        report.append(f"- Total Difference: Â¥{cross_check['total_difference']:,.0f}")
        report.append("")
        
        # Categorization analysis
        if not csv_data.empty:
            categorized_csv = self.categorize_transactions(csv_data)
            report.append("## Categorization Analysis (CSV)")
            
            category_summary = categorized_csv.groupby('category').agg({
                'amount': ['count', 'sum'],
                'confidence': 'mean'
            }).round(2)
            
            for category in category_summary.index:
                count = category_summary.loc[category, ('amount', 'count')]
                total = category_summary.loc[category, ('amount', 'sum')]
                avg_confidence = category_summary.loc[category, ('confidence', 'mean')]
                report.append(f"- **{category}**: {count} transactions, Â¥{total:,.0f} (avg confidence: {avg_confidence:.1%})")
            
            report.append("")
        
        # Discrepancies
        if cross_check['csv_only'] or cross_check['pdf_only']:
            report.append("## Discrepancies Found")
            
            if cross_check['csv_only']:
                report.append(f"### Transactions only in CSV ({len(cross_check['csv_only'])}):")
                for i, tx in enumerate(cross_check['csv_only'][:10]):  # Show first 10
                    report.append(f"- {tx['date'].strftime('%Y-%m-%d')} | {tx['description'][:30]}... | Â¥{tx['amount']:,.0f}")
                if len(cross_check['csv_only']) > 10:
                    report.append(f"... and {len(cross_check['csv_only']) - 10} more")
                report.append("")
            
            if cross_check['pdf_only']:
                report.append(f"### Transactions only in PDF ({len(cross_check['pdf_only'])}):")
                for i, tx in enumerate(cross_check['pdf_only'][:10]):  # Show first 10
                    report.append(f"- {tx['date'].strftime('%Y-%m-%d')} | {tx['description'][:30]}... | Â¥{tx['amount']:,.0f}")
                if len(cross_check['pdf_only']) > 10:
                    report.append(f"... and {len(cross_check['pdf_only']) - 10} more")
                report.append("")
        
        # Recommendations
        report.append("## Recommendations")
        if cross_check['reconciliation_status'] == 'major_discrepancy':
            report.append("- âš ï¸ **Major discrepancy detected** - Review both statements carefully")
            report.append("- Check for missing transactions in either source")
            report.append("- Verify transaction amounts and dates")
        elif cross_check['reconciliation_status'] == 'minor_discrepancy':
            report.append("- âš ï¸ **Minor discrepancy detected** - Likely due to timing differences")
            report.append("- Check for transactions near statement cut-off dates")
        else:
            report.append("- âœ… **Statements are reconciled** - Data integrity is good")
        
        report.append("- Review uncategorized transactions and add rules if needed")
        report.append("- Consider setting up automated reconciliation for future statements")
        
        return "\n".join(report)
    
    def export_categorized_data(self, df: pd.DataFrame, output_path: str) -> None:
        """Export categorized data to CSV."""
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Exported categorized data to {output_path}")


def main():
    """Main function to run the bank statement analyzer."""
    analyzer = BankStatementAnalyzer()
    
    # File paths
    csv_path = "/workspace/temp_repo/bank/statements/enavi202509(5734) (2).csv"
    pdf_path = "/workspace/temp_repo/bank/statements/statement_202509.pdf"
    
    try:
        # Parse statements
        print("ğŸ“Š Parsing bank statements...")
        csv_data = analyzer.parse_csv_statement(csv_path)
        pdf_data = analyzer.parse_pdf_statement(pdf_path)
        
        # Cross-check
        print("ğŸ” Cross-checking statements...")
        cross_check = analyzer.cross_check_statements(csv_data, pdf_data)
        
        # Generate report
        print("ğŸ“ Generating analysis report...")
        report = analyzer.generate_analysis_report(csv_data, pdf_data)
        
        # Save report
        with open("/workspace/bank_analysis_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("âœ… Analysis complete!")
        print(f"ğŸ“„ Report saved to: /workspace/bank_analysis_report.md")
        
        # Export categorized data
        if not csv_data.empty:
            categorized_data = analyzer.categorize_transactions(csv_data)
            analyzer.export_categorized_data(categorized_data, "/workspace/categorized_transactions.csv")
            print(f"ğŸ“Š Categorized data saved to: /workspace/categorized_transactions.csv")
        
        # Print summary
        print("\n" + "="*50)
        print("SUMMARY")
        print("="*50)
        print(f"CSV Transactions: {cross_check['csv_summary']['count']} | Total: Â¥{cross_check['csv_summary']['total']:,.0f}")
        print(f"PDF Transactions: {cross_check['pdf_summary']['count']} | Total: Â¥{cross_check['pdf_summary']['total']:,.0f}")
        print(f"Reconciliation: {cross_check['reconciliation_status']}")
        print(f"Difference: Â¥{cross_check['total_difference']:,.0f}")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        raise


if __name__ == "__main__":
    main()